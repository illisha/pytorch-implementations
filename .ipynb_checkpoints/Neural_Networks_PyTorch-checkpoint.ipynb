{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks with PyTorch \n",
    "\n",
    "Using the Pytorch `nn` module to build deep neural networks on the MNIST digit datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules \n",
    "import numpy as np \n",
    "import torch \n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Get the data \n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data \n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize(mean=0.5, std=0.5)\n",
    "                              ])\n",
    "\n",
    "\n",
    "# Download and load the tranining data \n",
    "trainset = datasets.MNIST('./data/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the trainloader to get the images \n",
    "data_iter = iter(trainloader)\n",
    "images, labels = data_iter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at one of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x195eadfc160>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[0].numpy().squeeze(), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a deep neural network for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid Activation\n",
    "def sigmoid(x):\n",
    " \n",
    "    return 1/(1 + torch.exp(-x))\n",
    "\n",
    "# Softmax Activation Function \n",
    "def softmax(x):\n",
    "    return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate some random data\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Flatten the input images \n",
    "inputs = images.view(images.shape[0], -1)\n",
    "\n",
    "# Define the size of each layer in our netowrk \n",
    "n_input = inputs.shape[1]\n",
    "n_hidden = 256 # Number of hidden units \n",
    "n_output = 10  # number of output units\n",
    "\n",
    "# Weights for inputs to hidden layer \n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "\n",
    "# Weights for hidden layer to outputs \n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "# Bias Terms \n",
    "B1 = torch.randn((1, n_hidden))\n",
    "B2 = torch.randn((1, n_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.2145e-03, 7.2968e-01, 4.3513e-05, 2.0525e-03, 3.6021e-02, 1.8656e-05,\n",
       "         3.0806e-06, 2.2357e-01, 2.7656e-06, 3.9524e-04],\n",
       "        [3.2211e-09, 4.6957e-03, 1.9462e-07, 6.0405e-06, 8.5535e-01, 1.3979e-01,\n",
       "         3.1062e-05, 4.2809e-05, 8.1511e-05, 6.8964e-07],\n",
       "        [7.0885e-07, 9.0611e-01, 1.8750e-08, 4.2969e-06, 9.0063e-02, 3.4439e-05,\n",
       "         5.2801e-05, 3.7370e-03, 1.7070e-09, 6.8304e-10],\n",
       "        [3.3240e-09, 1.0000e+00, 8.9573e-10, 1.8654e-10, 1.4356e-08, 2.5518e-11,\n",
       "         3.4602e-13, 1.0190e-12, 2.0078e-14, 2.9872e-12],\n",
       "        [3.3172e-07, 9.9266e-01, 5.1891e-07, 2.0927e-06, 2.8418e-03, 6.1460e-06,\n",
       "         2.2908e-04, 4.2400e-03, 5.4352e-12, 1.9744e-05],\n",
       "        [2.8935e-13, 2.4528e-05, 1.2525e-08, 5.9675e-07, 9.9997e-01, 3.0788e-11,\n",
       "         7.2302e-15, 1.1975e-08, 7.4299e-11, 9.7646e-10],\n",
       "        [1.0385e-13, 3.8793e-03, 7.8408e-12, 6.4179e-09, 9.9612e-01, 8.5272e-09,\n",
       "         1.6160e-09, 4.5544e-06, 1.6849e-10, 2.2109e-12],\n",
       "        [4.0672e-08, 6.8048e-02, 1.2236e-06, 3.8061e-08, 9.3182e-01, 1.2377e-04,\n",
       "         1.0498e-06, 3.1228e-06, 2.0059e-11, 5.4103e-10],\n",
       "        [1.6276e-10, 9.9717e-01, 3.8980e-14, 1.8296e-07, 2.8278e-03, 1.0742e-06,\n",
       "         1.0642e-12, 3.0373e-06, 5.6851e-09, 5.6337e-10],\n",
       "        [2.0781e-06, 5.6382e-02, 3.7677e-06, 3.8452e-07, 9.4324e-01, 2.5236e-04,\n",
       "         3.2481e-09, 5.7487e-05, 8.5267e-06, 5.0384e-05],\n",
       "        [3.9600e-09, 9.7941e-01, 5.2076e-08, 1.8828e-10, 2.6579e-03, 6.0182e-12,\n",
       "         1.9392e-07, 1.7935e-02, 1.9878e-12, 3.2418e-07],\n",
       "        [1.9122e-06, 3.4283e-01, 2.8850e-03, 1.0791e-05, 1.8852e-05, 3.9271e-01,\n",
       "         6.1849e-07, 2.6155e-01, 2.8558e-11, 7.4274e-07],\n",
       "        [3.6864e-09, 9.9973e-01, 9.0978e-11, 1.6517e-08, 1.6261e-04, 1.0843e-04,\n",
       "         1.2891e-10, 3.8080e-10, 3.1034e-10, 2.6540e-10],\n",
       "        [1.5667e-14, 3.1668e-07, 5.3261e-14, 2.2275e-12, 1.0000e+00, 2.7589e-12,\n",
       "         6.0365e-12, 7.9281e-07, 1.9626e-14, 6.6693e-13],\n",
       "        [2.3388e-06, 6.5098e-03, 1.3916e-09, 2.4627e-08, 9.9349e-01, 5.9429e-07,\n",
       "         3.6120e-12, 1.2159e-07, 8.8084e-08, 2.3735e-11],\n",
       "        [2.6487e-09, 1.2640e-01, 4.2224e-07, 1.1113e-03, 4.5824e-06, 2.7312e-10,\n",
       "         1.3097e-11, 8.7248e-01, 1.4567e-08, 2.8805e-08],\n",
       "        [6.2294e-10, 3.6231e-03, 1.3458e-08, 3.2322e-04, 6.0346e-05, 6.7341e-05,\n",
       "         2.4484e-07, 9.9569e-01, 1.0390e-07, 2.3292e-04],\n",
       "        [1.2733e-09, 9.9991e-01, 1.4040e-08, 1.1074e-06, 8.6728e-05, 2.7097e-08,\n",
       "         9.6859e-11, 7.9014e-10, 1.6161e-13, 1.1762e-12],\n",
       "        [1.5476e-07, 4.0243e-04, 4.3999e-06, 6.9873e-08, 5.6096e-02, 9.4347e-01,\n",
       "         9.3992e-11, 1.0790e-07, 3.2763e-08, 2.6491e-05],\n",
       "        [1.2744e-11, 1.0000e+00, 1.4795e-10, 7.5167e-09, 2.1827e-10, 4.4937e-08,\n",
       "         5.5324e-11, 3.2633e-06, 2.4470e-13, 1.8966e-11],\n",
       "        [1.9136e-07, 9.3566e-01, 6.8814e-05, 4.0920e-02, 1.8291e-02, 2.8008e-03,\n",
       "         3.9613e-06, 1.7764e-03, 2.2341e-07, 4.8194e-04],\n",
       "        [4.9296e-07, 8.6034e-01, 7.4589e-08, 1.2348e-10, 1.2994e-01, 9.7038e-03,\n",
       "         2.0686e-08, 1.4887e-05, 2.7623e-06, 4.6571e-09],\n",
       "        [3.8022e-10, 9.9932e-01, 4.9738e-09, 1.0490e-08, 6.0157e-04, 6.9489e-11,\n",
       "         3.0304e-11, 8.2571e-05, 4.0480e-12, 4.8764e-07],\n",
       "        [7.1994e-07, 9.9466e-01, 5.2479e-11, 9.0000e-09, 8.9258e-05, 7.3289e-05,\n",
       "         5.2109e-06, 5.1681e-03, 2.4903e-07, 2.0983e-14],\n",
       "        [1.3510e-07, 2.7194e-03, 7.3233e-04, 3.2048e-13, 9.3657e-01, 1.8695e-06,\n",
       "         8.6708e-11, 5.9972e-02, 7.8026e-08, 1.2596e-09],\n",
       "        [1.3320e-09, 8.1699e-07, 9.5795e-11, 1.9905e-07, 5.5780e-01, 1.1344e-07,\n",
       "         8.6252e-06, 4.4205e-01, 1.9110e-10, 1.3674e-04],\n",
       "        [1.4363e-10, 1.2315e-03, 3.8290e-06, 2.9524e-01, 7.8800e-03, 6.9564e-01,\n",
       "         8.7631e-08, 3.0894e-06, 1.7799e-08, 2.6145e-07],\n",
       "        [9.1017e-09, 8.7623e-01, 1.3197e-10, 5.5664e-09, 1.2137e-01, 2.4022e-03,\n",
       "         8.1631e-10, 2.0117e-09, 6.1971e-08, 2.5923e-10],\n",
       "        [6.1062e-12, 1.2401e-01, 2.5994e-07, 1.7063e-07, 8.7586e-01, 4.6908e-05,\n",
       "         8.3564e-05, 3.3749e-07, 3.2621e-09, 3.4193e-10],\n",
       "        [3.4948e-11, 9.5982e-01, 3.0858e-06, 4.3215e-09, 3.5577e-02, 6.6305e-07,\n",
       "         5.6894e-09, 4.6007e-03, 2.4845e-11, 3.9842e-09],\n",
       "        [4.6066e-09, 1.0000e+00, 3.0859e-12, 1.2933e-10, 6.8213e-07, 2.0165e-06,\n",
       "         1.6608e-10, 3.4923e-07, 1.4908e-11, 2.2657e-08],\n",
       "        [1.3045e-04, 1.5403e-02, 4.2857e-04, 2.2042e-02, 1.0512e-01, 4.5455e-04,\n",
       "         3.7164e-09, 8.5632e-01, 2.6185e-05, 6.5670e-05],\n",
       "        [1.0030e-11, 1.8801e-03, 5.8123e-08, 2.5522e-06, 3.4667e-04, 1.8302e-05,\n",
       "         4.4950e-07, 9.9775e-01, 2.7065e-12, 3.5790e-08],\n",
       "        [3.7394e-13, 1.8499e-07, 2.4807e-16, 1.5738e-07, 1.0000e+00, 1.5514e-07,\n",
       "         9.2689e-12, 9.2155e-08, 2.9229e-08, 2.9626e-10],\n",
       "        [1.1140e-08, 2.7394e-05, 3.2746e-12, 4.6903e-10, 9.9997e-01, 6.6623e-06,\n",
       "         9.7029e-13, 7.1883e-10, 3.5815e-10, 8.9555e-13],\n",
       "        [3.5374e-14, 6.7144e-07, 5.7709e-11, 4.2096e-07, 1.0000e+00, 5.1584e-09,\n",
       "         8.9286e-13, 2.6217e-07, 1.1384e-13, 3.1377e-12],\n",
       "        [2.2478e-06, 6.1742e-01, 1.5724e-06, 1.3611e-06, 3.8020e-01, 2.3674e-03,\n",
       "         1.1581e-10, 8.9350e-06, 6.5887e-09, 6.1066e-09],\n",
       "        [3.6552e-08, 9.9948e-01, 9.1262e-15, 1.3467e-06, 5.1780e-04, 6.4198e-10,\n",
       "         1.6583e-09, 2.8529e-10, 4.2529e-14, 1.3056e-11],\n",
       "        [5.1241e-09, 9.9759e-01, 5.2022e-08, 2.7049e-08, 2.2449e-03, 1.0085e-10,\n",
       "         4.5391e-10, 1.6240e-04, 2.8355e-12, 3.3836e-09],\n",
       "        [5.9272e-08, 1.9206e-01, 2.7503e-06, 9.4917e-08, 3.5023e-02, 7.7211e-01,\n",
       "         6.5541e-09, 6.8103e-04, 7.6082e-06, 1.1138e-04],\n",
       "        [4.0226e-10, 8.2095e-01, 2.3953e-09, 1.9434e-08, 1.7897e-01, 6.8225e-05,\n",
       "         8.9777e-06, 1.7132e-06, 2.5101e-13, 9.1575e-11],\n",
       "        [8.6024e-11, 3.9564e-06, 4.7219e-05, 1.2015e-06, 6.4049e-02, 5.2660e-03,\n",
       "         7.9645e-06, 9.3023e-01, 2.7227e-04, 1.2312e-04],\n",
       "        [3.6686e-08, 5.1845e-04, 1.0572e-06, 3.0893e-06, 1.8679e-05, 2.6375e-06,\n",
       "         2.6732e-07, 9.9944e-01, 3.4492e-09, 1.5901e-05],\n",
       "        [7.7280e-09, 1.5405e-02, 8.9713e-09, 1.0754e-06, 9.8434e-01, 2.5220e-04,\n",
       "         5.3309e-12, 6.9742e-07, 7.2720e-11, 5.4731e-10],\n",
       "        [2.4236e-06, 3.5070e-05, 4.3674e-07, 6.1535e-05, 9.5540e-01, 7.8144e-08,\n",
       "         8.8362e-07, 4.4503e-02, 1.3479e-08, 3.5227e-07],\n",
       "        [6.1389e-09, 5.8049e-03, 1.2734e-04, 1.4436e-09, 9.8972e-01, 2.8706e-07,\n",
       "         1.4070e-06, 4.3479e-03, 2.3333e-08, 6.9555e-09],\n",
       "        [2.7791e-07, 4.5304e-02, 2.1691e-04, 6.0571e-07, 1.6270e-01, 2.7052e-04,\n",
       "         8.3836e-03, 3.4817e-01, 4.3489e-01, 6.7629e-05],\n",
       "        [7.0797e-10, 1.6809e-03, 3.5763e-06, 3.8384e-06, 1.1137e-03, 4.5537e-04,\n",
       "         1.9136e-14, 9.9674e-01, 1.4598e-09, 3.5361e-07],\n",
       "        [2.5291e-07, 2.4648e-04, 5.3390e-07, 1.5931e-12, 9.9973e-01, 1.5511e-05,\n",
       "         2.8345e-09, 9.3481e-06, 3.9480e-08, 1.7959e-09],\n",
       "        [1.8751e-08, 9.9953e-01, 2.3980e-11, 4.8127e-09, 4.6812e-04, 3.0468e-08,\n",
       "         2.5625e-09, 1.8478e-09, 1.8493e-10, 9.7412e-12],\n",
       "        [2.5278e-06, 9.3946e-01, 9.2031e-10, 5.1110e-07, 6.0260e-02, 2.7605e-04,\n",
       "         1.7216e-07, 1.6627e-07, 1.2899e-11, 6.6186e-13],\n",
       "        [2.0371e-12, 5.6630e-03, 3.3180e-05, 6.7597e-09, 9.9429e-01, 8.8517e-06,\n",
       "         2.5436e-06, 9.5296e-08, 7.0627e-11, 9.1467e-11],\n",
       "        [1.0426e-07, 9.2380e-01, 6.1240e-08, 4.0343e-06, 7.0446e-02, 3.9041e-03,\n",
       "         2.9786e-06, 1.4630e-03, 2.8050e-08, 3.8107e-04],\n",
       "        [2.1809e-06, 3.0872e-03, 2.7080e-08, 2.1622e-05, 7.0214e-04, 2.0954e-03,\n",
       "         3.2390e-06, 9.9409e-01, 1.9982e-07, 1.3418e-07],\n",
       "        [1.7858e-01, 7.5748e-01, 1.3221e-05, 2.3980e-08, 6.3922e-02, 2.2759e-06,\n",
       "         5.8636e-10, 2.0983e-08, 1.8581e-08, 6.7296e-08],\n",
       "        [5.9187e-06, 9.9988e-01, 3.7316e-09, 9.9160e-08, 6.2376e-09, 2.4906e-06,\n",
       "         2.6399e-12, 9.6996e-05, 2.4868e-09, 1.5536e-05],\n",
       "        [2.0714e-09, 7.5579e-01, 4.8648e-06, 4.3157e-08, 2.4110e-01, 3.0875e-03,\n",
       "         2.6138e-09, 1.2506e-05, 3.2214e-10, 1.9013e-09],\n",
       "        [1.8503e-06, 2.0409e-04, 1.3863e-08, 9.1747e-10, 9.7418e-01, 5.9774e-05,\n",
       "         3.4747e-13, 2.5534e-02, 1.7199e-05, 1.2726e-06],\n",
       "        [9.3017e-05, 6.5404e-01, 1.7575e-06, 9.7474e-03, 2.8762e-02, 1.4912e-03,\n",
       "         4.9454e-04, 3.0537e-01, 3.0773e-09, 3.6179e-07],\n",
       "        [1.3878e-08, 8.4142e-05, 8.9273e-12, 3.7719e-08, 9.9636e-01, 1.6438e-04,\n",
       "         3.0032e-07, 3.3953e-03, 2.5198e-12, 7.5192e-07],\n",
       "        [6.5474e-15, 9.9999e-01, 6.9182e-13, 3.6129e-11, 4.4860e-06, 1.2579e-09,\n",
       "         3.2558e-15, 2.5086e-06, 6.6182e-14, 2.5153e-09],\n",
       "        [2.3055e-07, 1.2370e-03, 5.0707e-13, 5.3180e-10, 9.9876e-01, 5.7439e-07,\n",
       "         3.8540e-13, 1.2959e-07, 2.3520e-11, 5.1835e-08],\n",
       "        [5.5754e-09, 2.3018e-01, 2.5257e-03, 7.6252e-05, 5.6015e-01, 1.3502e-02,\n",
       "         1.2193e-06, 1.9321e-01, 2.3097e-09, 3.4840e-04],\n",
       "        [1.7245e-09, 1.0483e-03, 3.4325e-09, 8.5695e-09, 9.9895e-01, 1.1755e-06,\n",
       "         3.5797e-11, 7.5306e-10, 3.5209e-11, 1.6425e-09]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcualte the outputs \n",
    "h = sigmoid(torch.mm(inputs, W1) + B1)\n",
    "probs = softmax(torch.mm(h, W2) + B2)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "# Check prob details \n",
    "print(probs.shape)\n",
    "print(probs.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Neural Network with PyTorch\n",
    "\n",
    "PyTorch provides the `nn` module that makes building networks much simpler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitNeuralNetwork(nn.Module): \n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create linear transformation functions\n",
    "        # Inputs to hidden layer linear transformation \n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        \n",
    "        # Output layer, 10 units for each digit \n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "        # Dfine the sigmoid and softmax activation functions\n",
    "        self.sigmoid =nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Pass the input tensor through each operation \n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=256, out_features=10, bias=True)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DigitNeuralNetwork()\n",
    "model.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elementwise functions**\n",
    "\n",
    "Networks can be defined somewhat more concisely and clearly using the `torch.nn.functional` module. This is the most common way of defining networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FunctionalNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create linear transformation functions\n",
    "        # Inputs to hidden layer linear transformation \n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        \n",
    "        # Output layer, 10 units for each digit \n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Hidden layer with the sigmoid activation \n",
    "        x = F.sigmoid(self.hidden(x))\n",
    "        \n",
    "        # Output layer with softmax activation \n",
    "        x = F.softmax(self.output(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionalNetwork(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FunctionalNetwork()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a deeper neural network with two hidden layes and relu layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create linear transformation functions\n",
    "        # Inputs to hidden layer linear transformation \n",
    "        self.hidden1 = nn.Linear(784, 128)\n",
    "        \n",
    "        # Create the second hidden layer\n",
    "        self.hidden2 = nn.Linear(128, 64)\n",
    "        \n",
    "        # Output layer, 10 units for each digit \n",
    "        self.output = nn.Linear(64, 10)\n",
    "        \n",
    "        # Create the final loss layer \n",
    "        self.loss = nn.Linear(10, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Hidden layers with the relu activation \n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        \n",
    "        # Output layer with softmax activation \n",
    "        x = F.softmax(self.output(x), dim=1)\n",
    "        \n",
    "        # Loss layer with cross entropy \n",
    "        x = F.cross_entropy(self.loss(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (hidden2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (loss): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2969, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "## Build a feed forward neural \n",
    "model = nn.Sequential(nn.Linear(784, 128), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Linear(128, 64), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Linear(64, 10))\n",
    "\n",
    "# Define the loss \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Get our data \n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "# Flatten images \n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Forward pass, get our logits\n",
    "logits = model(images)\n",
    "\n",
    "# Calculcate the loss\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model with log softmax updated activation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3014, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "## Build a feed forward neural \n",
    "model = nn.Sequential(nn.Linear(784, 128), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Linear(128, 64), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Linear(64, 10), \n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "# Define the loss as the negative log likelihood loss \n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Get our data \n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "# Flatten the images \n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Forward pass, get our logits\n",
    "log_probs = model(images)\n",
    "\n",
    "# Calculcate the loss\n",
    "loss = criterion(log_probs, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the backward propagation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before BackProp:  None\n",
      "After BackProp:  tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0011, -0.0011, -0.0011,  ..., -0.0011, -0.0011, -0.0011],\n",
      "        [-0.0006, -0.0006, -0.0006,  ..., -0.0006, -0.0006, -0.0006],\n",
      "        ...,\n",
      "        [ 0.0014,  0.0014,  0.0014,  ...,  0.0014,  0.0014,  0.0014],\n",
      "        [ 0.0010,  0.0010,  0.0010,  ...,  0.0010,  0.0010,  0.0010],\n",
      "        [-0.0023, -0.0023, -0.0023,  ..., -0.0023, -0.0023, -0.0023]])\n"
     ]
    }
   ],
   "source": [
    "# Using the same model as before \n",
    "print(\"Before BackProp: \", model[0].weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(\"After BackProp: \", model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is none because we haven't actually calculated the gradients before but now we have. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimiser\n",
    "\n",
    "Optimisers will be used to update the weights with the gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Optimiser require the parameters to optimise and a learning rate \n",
    "optimiser = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimiser step to update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights:  Parameter containing:\n",
      "tensor([[ 0.0264, -0.0130,  0.0119,  ..., -0.0055, -0.0300, -0.0202],\n",
      "        [-0.0071, -0.0131, -0.0274,  ..., -0.0188, -0.0147, -0.0061],\n",
      "        [-0.0320,  0.0095,  0.0308,  ...,  0.0299,  0.0205, -0.0105],\n",
      "        ...,\n",
      "        [ 0.0118, -0.0065,  0.0242,  ...,  0.0309, -0.0300, -0.0337],\n",
      "        [ 0.0044,  0.0053, -0.0322,  ...,  0.0301, -0.0227, -0.0238],\n",
      "        [ 0.0213,  0.0064,  0.0029,  ..., -0.0290,  0.0183,  0.0176]],\n",
      "       requires_grad=True)\n",
      "Gradient:  tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-2.1524e-03, -2.1524e-03, -2.1524e-03,  ..., -2.1524e-03,\n",
      "         -2.1524e-03, -2.1524e-03],\n",
      "        [ 2.7586e-05,  2.7586e-05,  2.7586e-05,  ...,  2.7586e-05,\n",
      "          2.7586e-05,  2.7586e-05],\n",
      "        ...,\n",
      "        [-8.2186e-05, -8.2186e-05, -8.2186e-05,  ..., -8.2186e-05,\n",
      "         -8.2186e-05, -8.2186e-05],\n",
      "        [-5.7988e-03, -5.7988e-03, -5.7988e-03,  ..., -5.7988e-03,\n",
      "         -5.7988e-03, -5.7988e-03],\n",
      "        [-2.5355e-03, -2.5355e-03, -2.5355e-03,  ..., -2.5355e-03,\n",
      "         -2.5355e-03, -2.5355e-03]])\n",
      "Updated weights:  Parameter containing:\n",
      "tensor([[ 0.0264, -0.0130,  0.0119,  ..., -0.0055, -0.0300, -0.0202],\n",
      "        [-0.0070, -0.0131, -0.0273,  ..., -0.0187, -0.0146, -0.0061],\n",
      "        [-0.0320,  0.0095,  0.0308,  ...,  0.0299,  0.0205, -0.0105],\n",
      "        ...,\n",
      "        [ 0.0118, -0.0065,  0.0242,  ...,  0.0309, -0.0300, -0.0337],\n",
      "        [ 0.0044,  0.0054, -0.0321,  ...,  0.0301, -0.0226, -0.0237],\n",
      "        [ 0.0213,  0.0065,  0.0029,  ..., -0.0290,  0.0183,  0.0176]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Look how the weights will change\n",
    "print(\"Initial weights: \", model[0].weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64, 784)\n",
    "\n",
    "# Clear the gradients to not have them accumulated \n",
    "optimiser.zero_grad()\n",
    "\n",
    "# Passes \n",
    "output = model.forward(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print(\"Gradient: \", model[0].weight.grad)\n",
    "\n",
    "# Take an update step and few new weights \n",
    "optimiser.step()\n",
    "print(\"Updated weights: \", model[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model \n",
    "\n",
    "1. Make a forward pass \n",
    "2. Use the network to calculate the loss \n",
    "3. Perform a backward pass through the network with `loss.backward()` to calculate the gradients \n",
    "4. Take a step with the optimiser to update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.8664949874379742\n",
      "Training loss: 0.7973342202683248\n",
      "Training loss: 0.5011892105534133\n",
      "Training loss: 0.4173971832847036\n",
      "Training loss: 0.37813867097978654\n"
     ]
    }
   ],
   "source": [
    "## Build a feed forward neural \n",
    "model = nn.Sequential(nn.Linear(784, 128), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Linear(128, 64), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Linear(64, 10), \n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimiser = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs=5\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    running_loss = 0    # Running loss for each epoch\n",
    "    for images, labels in trainloader:\n",
    "        \n",
    "        # Flatten the images \n",
    "        images = images.view(images.shape[0], -1)\n",
    "        \n",
    "        # Clear the gradients to not have them accumulated \n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        # Forward pass \n",
    "        output = model.forward(images)\n",
    "        \n",
    "        # Calculate the loss \n",
    "        loss = criterion(output, labels)\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Backward Prop step \n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights \n",
    "        optimiser.step()\n",
    "    else: \n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
